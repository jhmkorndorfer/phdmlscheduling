#+TITLE: Labbook Jonas PhD on MLS 
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Jonas(J) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)
* Description of the experimental platform
  + Describe 
  + Describe 

* 2016-04-04 EXAMPLE GRAPHS R
** Just Organizing
Load the data:

#+begin_src R :results output :session :exports both
  df2 <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", sep=" ");
  names(df2) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine");
  df2 <- df2[df2$Disk == "SSD", ];
  df2 <- df2[df2$Scalability == "Strong", ];
  df2 <- df2[df2$Pinned == "Pin", ];
#+end_src

#+RESULTS:

#+begin_src R :results output :session :exports both
summary (df2[df2$Thread == 1,]$Time);
sequential_time = mean(df2[df2$Thread == 1 & df2$Machine == "bali",]$Time);
sequential_time
#+end_src

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
:   484.5   490.3   707.4   644.1   792.0   793.0
: [1] 496.0953

#+begin_src R :results output :session :exports both
library(plyr)
cdata <- ddply(df2[df2$Threads != 1,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               sequential_time = mean(df2[df2$Threads == 1 & df2$Machine == Machine,]$Time),
               speedup_mean = sequential_time/mean(Time),
               speedup_se = 3 * sd(sequential_time/Time) / sqrt(N));
cdata
#+end_src

#+RESULTS:
#+begin_example
  Threads Disk Pinned Machine  N time_mean    time_se sequential_time
1       2  SSD    Pin    bali 30  289.7503 6.05618067        496.0953
2       2  SSD    Pin  turing 30  388.4730 0.39249323        792.0303
3       4  SSD    Pin    bali 30  151.7505 0.50277448        496.0953
4       4  SSD    Pin  turing 30  222.2863 0.07860519        792.0303
5       8  SSD    Pin    bali 30   79.9273 0.22343101        496.0953
6       8  SSD    Pin  turing 30  182.1544 5.69851655        792.0303
7      16  SSD    Pin    bali 30   53.0513 0.15579382        496.0953
8      16  SSD    Pin  turing 30  743.2732 9.97565459        792.0303
  speedup_mean  speedup_se
1     1.712147 0.035278607
2     2.038830 0.002059055
3     3.269150 0.010766602
4     3.563109 0.001260241
5     6.206832 0.017204858
6     4.348126 0.144101927
7     9.351238 0.027443202
8     1.065598 0.015460372
#+end_example

#+begin_src R :results output :session :exports both
s = c(1, 2, 4, 8, 16)
e = c(1, 1, 1, 1, 1)
ideal = data.frame(s, s, s, e)
names(ideal) <- c("P", "Threads", "speedup_mean", "Efficiency")
ideal$Threads <- as.integer(ideal$Threads);
ideal$Pinned <- NA
ideal$Machine <- NA
ideal
#+end_src

#+RESULTS:
:    P Threads speedup_mean Efficiency Pinned Machine
: 1  1       1            1          1     NA      NA
: 2  2       2            2          1     NA      NA
: 3  4       4            4          1     NA      NA
: 4  8       8            8          1     NA      NA
: 5 16      16           16          1     NA      NA

#+begin_src R :results output graphics :file img/speedup-with-variability-jonas-diss.pdf :exports both :width 6 :height 4 :session

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=speedup_mean, color=Machine)) +
      geom_line(data=ideal, aes(group=Machine)) +
      geom_point(size=4, alpha=.5) +
      geom_line(aes(group=Machine)) +
      theme_bw() +
      ylim(0,NA) +
     #xlim(0,NA) +
      scale_x_continuous(breaks=s) +
      geom_errorbar(aes(ymax = speedup_mean+speedup_se, ymin=speedup_mean-speedup_se), width=.5);
p
#+end_src

#+RESULTS:
[[file:img/speedup-with-variability-jonas-diss.pdf]]

** Example
#+begin_src R :results output graphics :file img/finalImgs/strong-speedup-with-variabilityTuring6-jonas-diss.pdf :exports both :width 6 :height 4 :session
df3 <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", sep=" ");
names(df3) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine");
df3 <- df3[df3$Disk == "SSD", ];
df3 <- df3[df3$Scalability == "Strong", ];
df3 <- df3[df3$Pinned == "Free", ];

summary (df3[df3$Thread == 1,]$Time);
sequential_time = mean(df3[df3$Thread == 1 & df3$Machine == "bali",]$Time);
library(plyr);
cdata <- ddply(df3[df3$Threads != 1,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               sequential_time = min(df3[df3$Threads == 1 & df3$Machine == Machine,]$Time),
               SpeedUp = sequential_time/min(Time),
               speedup_se = 3 * sd(sequential_time/Time) / sqrt(N));

s = c(1, 2, 4, 8, 16);
e = c(1, 1, 1, 1, 1);
ideal = data.frame(s, s, s, e);
names(ideal) <- c("P", "Threads", "SpeedUp", "Efficiency");
ideal$Threads <- as.integer(ideal$Threads);
ideal$Pinned <- NA;
ideal$Machine <- NA;

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=SpeedUp, color=Machine)) +
  geom_line(data=ideal, aes(group=Machine), color="black",  alpha=.7) +
  geom_point(size=2, alpha=.9) +
  geom_line(aes(group=Machine)) +
  ggtitle("SpeedUp - Strong Scaling") +
  theme_bw() +
  scale_y_continuous(breaks=s) +
  scale_x_continuous(breaks=s) +
  geom_errorbar(aes(ymax = SpeedUp+speedup_se, ymin=SpeedUp-speedup_se), width=.5);
print(p);

#+end_src

#+RESULTS:
[[file:img/finalImgs/strong-speedup-with-variabilityTuring6-jonas-diss.pdf]]

** Example
#+begin_src R :results output graphics :file img/finalImgs/strong-efficiecy-with-variabilityTuring6-jonas-diss.pdf :exports both :width 6 :height 4 :session
df <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", header=FALSE, sep=" ")
names(df) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine")
df2 <- df[df$Disk == "SSD" & df$Scalability == "Strong" & df$Pinned == "Free" ,]
library(plyr)
cdata <- ddply(df2[df2$Threads != 20,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               seq_time = min(df2[df2$Threads == 1 & df2$Pinned == Pinned & df2$Machine == Machine,]$Time),
               SpeedUp = seq_time/min(Time),
               Efficiency= min(SpeedUp/Threads),
               Efficiency_se = 3 * sd((seq_time/Time)/Threads) / sqrt(N));

print (cdata)
s = c(1, 2, 4, 8, 16)
e = c(1, 1, 1, 1, 1)
sss = c(1, 0.85, 0.7, 0.55, 0.4, 0.25, 0.1)
ideal = data.frame(s, s, e)
names(ideal) <- c("P", "Threads", "Efficiency")
ideal$Threads <- as.integer(ideal$Threads);
ideal$Machine <- NA

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=Efficiency, color=Machine)) +
  geom_line(data=ideal, aes(group=Machine), alpha=.7) +
  geom_point(size=2, alpha=.9) +
  geom_line(aes(group=Machine)) +
  theme_bw() +
  ggtitle("Efficiency - Strong Scaling") +
  scale_x_continuous(breaks=s) +
  scale_y_continuous(breaks=sss) +
  geom_errorbar(aes(ymax = Efficiency+Efficiency_se, ymin=Efficiency-Efficiency_se), width=.5);
print(p);
#+end_src

#+RESULTS:
[[file:img/finalImgs/strong-efficiecy-with-variabilityTuring6-jonas-diss.pdf]]

* 2016-04-04 To export to PDF

Next code block will be exported to =file.png=

#+begin_src R :results output graphics :file file.png :exports both :width 600 :height 400 :session

#+end_src

Next code block will be exported to =file.pdf=. See witdth and height in
incles, not pixels.

#+begin_src R :results output graphics :file file.pdf :exports both :width 6 :height 4 :session

#+end_src
* 2018-05-23 Papers PhD Brief
** 2018-05-03 Mixed Task Scheduling and Resource Allocation Problems 2000
(a bit confuse article)
The paper presents a constraint-based approach for mixed task
scheduling and resource problem. Two types of constraints: temportal
constrained problem and/or time and resource constrained problem.
 
However, since resource constraints are modelled by temporal
constraints, the semantics of the constraints is forgotten 
and the algorithm cannot consider the specificity of 
TSRA (see section 4).

*REF:*
@article{huguet2000mixed,
  title={Mixed task scheduling and resource allocation problems},
  author={Huguet, Marie-Jos{\'e} and Lopez, Pierre},
  journal={Proceedings of CP-AI-ORâ€™00, Paderborn, Germany},
  pages={71--79},
  year={2000}
}
** 2018-05-03 Multiscale computing (From the Academy) PNAS 2001

Wavelet approach
Multiscale Computer Graphics

Paper objective: describe how MSC can help scientists understand
complex data through two examples: one from acoustical signal
processing and second from computer graphics.

Some phrases:
"the field has undergone tremendous advances during the past decade
because of the increase in inexpensive, powerful hardware." 
"MSC is used in many disciplines, but its presence is often obscured,
because it appears unter several different names depending on the
field of application."

*REF:*
@article {Kobayashi12344,
	author = {Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
	title = {Multiscale computing},
	volume = {98},
	number = {22},
	pages = {12344--12345},
	year = {2001},
	doi = {10.1073/pnas.231384298},
	publisher = {National Academy of Sciences},
	abstract = {Multiscale computing (MSC) involves the computation, manipulation, and analysis of information at different resolution levels. Widespread use of MSC algorithms and the discovery of important relationships between different approaches to implementation were catalyzed, in part, by the recent interest in wavelets. We present two examples that demonstrate how MSC can help scientists understand complex data. The first is from acoustical signal processing and the second is from computer graphics. MSC,multiscale computing},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/98/22/12344},
	eprint = {http://www.pnas.org/content/98/22/12344.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

** 2018-05-07 Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and Practice 2007
Look again
Look again page 6


Some phrases
"A chief characteristic of next-generation computing systems is the
prevalence of parallelism at multiple levels of granularity."page 1 - 1

"the overall goal of the scheduler is to map tasks to processors so
that dependencies in the graph are not violated and execution time
and/or space is minimized." page 2 - 1

"The idea of multiscale scheduling, then, is to integrate cooperative 
and competitive scheduling methods into a unified framework that takes
account of both levels to minimize ERT of competitively scheduled
jobs while permitting their decomposition into cooperatively scheduled
tasks." page 2 - 5



*REF:*
@article{blelloch2007multiscale,
  title={Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and in Practice},
  author={Blelloch, Guy E and Blum, Lenore and Harchol-Balter, Mor and Harper, Robert},
  year={2007}
}
** 2018-05-07 Two level adaptive scheduling JSSPP 2009 - *Not Working*
** 2018-05-07 Optimized Grid Scheduling Using Two Level Decision Algorithm (TLDA) 2010

Combined schedulling starting by ACO (Ant Colony Optimization) and 
then GA (Genetic algoritim)

"TLDA (Two Level Decision Algorithm) shows improvement over nature
based algorithms applied independently"
"The overhead of decision making time can be neglected as 
compared to improvement in execution time"

The work shows that the overhead caused by the decision phase of the
schedulling can be neglected considering the execution time improvement.


*REF:*
@inproceedings{umale2010optimized,
  title={Optimized grid scheduling using two level decision algorithm (TLDA)},
  author={Umale, Jayant and Mahajan, Sunita},
  booktitle={Parallel Distributed and Grid Computing (PDGC), 2010 1st International Conference on},
  pages={78--82},
  year={2010},
  organization={IEEE}
}
** 2018-05-08 Compilers and More: Programming at Exascale - report - 2011
Levels of paralelism
** 2018-05-09 A multi-level scheduler for batch jobs on grids - 2011

*PAPER WITH GOOD STRUCTURE*
  
They proposes a two-level scheduler for dynamically scheduling a
continuous stream of sequential and multi-threaded batch jobs on
grids, "made up of interconnected clusters of heterogeneous
single-processor and/or symmetric multi- processor machines."

"At the top of the hierarchy a lightweight meta-scheduler (MS) clas-
sifies incoming jobs according to their requirements, and schedules them among the
underlying resources balancing the workload. At cluster level a Flexible Backfilling
algorithm carries out the job machine associations by exploiting dynamic informa-
tion about the environment."

"In this paper we describe the study conducted to develop a two-level queue-based
scheduling framework to schedule a continuous stream of independent batch jobs in
grids."

"Moreover, *our scheduler can be classified as static*, this meaning that jobs are as-
signed to the appropriate resources before their execution begins. Once started, they
run on the same resources without interruption."

"The OAR and KOALA queue-based multi-level schedulers are described respec-
tively in 13 and 14. OAR is based on backfilling."

*Meta-schedule.* Defines which job goes to which cluster based on two
functions *Load* and *Ordering*. *Load* aims to dispatch jobs among clusters
considering their workload by assigning a job to the less loaded
cluster. *Ordering* considers the priority of the jobs to balance the number of jobs
with same priority in each cluster queue.

*Local-scheduler.* "Flexible Backfilling algorithm
that selects the machines suitable to perform a job considering the number of proces-
sors and the licenses exploitable on a machine."

"MS Heuristics: MS classifies submitted jobs and dispatches them to LSs. At LS
level, scheduling decision are made by means of a Flexible Backfilling algorithm,
which exploits job priorities computed by MS. Any job prioritization is performed
at LS level. Higher the job priority is, higher the position of the job in LSsâ€™ queues
is."

"The proposed solution aims to schedule arriving jobs balancing the
clusters workload, respecting the job running require-ments 
and deadlines, and optimizing the utilization of hardware and software
resources."

"The conducted simulation tests demonstrated that the investigated
solution can be a viable one. In particular, we show that using a
lightweight component like MS joined with light-ening LSs, carries 
out good results as using more complex LSs."


Published online: 22 February 2011
Â© Springer Science+Business Media, LLC 2011
*REF:*
@article{pasquali2011multi,
  title={A multi-level scheduler for batch jobs on grids},
  author={Pasquali, Marco and Baraglia, Ranieri and Capannini, Gabriele and Ricci, Laura and Laforenza, Domenico},
  journal={The Journal of Supercomputing},
  volume={57},
  number={1},
  pages={81--98},
  year={2011},
  publisher={Springer}
}
** 2018-05-12 A Hierarchical Approach for Load Balancing on Parallel Multi-core Systems 2012 International Conference on Parallel Processing
"We introduce N UCO LB, a topology-aware load balancer that focuses on
redistributing work while reducing communication costs among and
within compute nodes."

"The NUMA architecture is a scalable solution to alleviate the memory
wall problem, and to provide better scalability for multi-core compute
nodes. A NUMA ar- chitecture features distributed shared memory with
asymmetric memory access costs."

"We introduce the N UCO LB load balancer, which combines information
about the NUMA multi-core topology, the interconnection network
latencies and statistics of the application captured during
execution."

"Thus, our objective for load balancing is to both maximize the use of
the cores (minimize idleness) and also minimize the communication
costs experienced by the application (maximize locality nd affinity)"

"On these systems, an action taken by the load balancer to equalize
the load on the available processors may actually make the overall 
performance worse by increasing the communication time."

"The load balancer needs to know how far from each other the tasks are
mapped, so that it can reduce the communication costs."

"In order to efficiently utilize a parallel machine, a load balancing
algorithm must consider not only the computational load of the
application, but also the existing asymmetries in memory latencies and
bandwidth, and network communication costs."

*REF:*
@inproceedings{pilla2012hierarchical,
  title={A hierarchical approach for load balancing on parallel multi-core systems},
  author={Pilla, Laercio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V},
  booktitle={Parallel Processing (ICPP), 2012 41st International Conference on},
  pages={118--127},
  year={2012},
  organization={IEEE}
}
** 2018-05-15 A Combined Dual-stage Framework for Robust Scheduling of Scientific Applications in Heterogeneous Environments with Uncertain Availability 2012
"A dual-stage framework is proposed in this paper to evaluate the
robustness of efficient resource allocation and dynamic load balancing
of scientific applications in heterogeneous computing environments with uncertain availability."

"The work presented herein demonstrates that using robust resource
allocation (RA) heuristics and application load balancing via
dynamic loop scheduling (DLS) techniques, in concert, will enhance the
execution of computationally intensive scientific applications in
uncertain heterogeneous systems."


"The goal of this research is to assign applications to heterogeneous
computing systems and execute them in such a way that all applications
complete before a common deadline, and their completion times are
robust against uncertainty in input data and system availability."

"Contribution. The main contribution of this paper is the design of an
intelligent two-stage framework to solve the problem of allocating
resources to applications to maximize the probability that the
applications can complete by a common deadline given uncertainty in
the input data and system availability, including developing a
mathematical model of this environment."

*REF:*
@inproceedings{ciorba2012combined,
  title={A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability},
  author={Ciorba, Florina M and Hansen, Timothy and Srivastava, Srishti and Banicescu, Ioana and Maciejewski, Anthony A and Siegel, Howard Jay},
  booktitle={Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW), 2012 IEEE 26th International},
  pages={193--207},
  year={2012},
  organization={IEEE}
}
** 2018-05-15 Heuristics for Robust Allocation of Resources to Parallel Applications with Uncertain Execution Times in Heterogeneous Systems with Uncertain Availability 2014



*REF:*
@inproceedings{hansen2014heuristics,
  title={Heuristics for robust allocation of resources to parallel applications with uncertain execution times in heterogeneous systems with uncertain availability},
  author={Hansen, Timothy and Ciorba, Florina M and Maciejewski, Anthony A and Siegel, Howard Jay and Srivastava, Srishti and Banicescu, Ioana},
  booktitle={Proceedings of the World Congress on Engineering},
  volume={1},
  year={2014}
}
** 2018-05-15 An adaptive and hierarchical task scheduling scheme for multi-core clusters 2014
*REF:*
@article{wang2014adaptive,
  title={An adaptive and hierarchical task scheduling scheme for multi-core clusters},
  author={Wang, Yizhuo and Zhang, Yang and Su, Yan and Wang, Xiaojun and Chen, Xu and Ji, Weixing and Shi, Feng},
  journal={Parallel Computing},
  volume={40},
  number={10},
  pages={611--627},
  year={2014},
  publisher={Elsevier}
}

** 2018-05-15 Multi-stage resource-aware scheduling for data centers with heterogeneous servers 2018

*REF:*
@article{tran2018multi,
  title={Multi-stage resource-aware scheduling for data centers with heterogeneous servers},
  author={Tran, Tony T and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G and Beck, J Christopher},
  journal={Journal of Scheduling},
  volume={21},
  number={2},
  pages={251--267},
  year={2018},
  publisher={Springer}
}
* All BibTeX REFs
*REF:*
@article{tran2018multi,
  title={Multi-stage resource-aware scheduling for data centers with heterogeneous servers},
  author={Tran, Tony T and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G and Beck, J Christopher},
  journal={Journal of Scheduling},
  volume={21},
  number={2},
  pages={251--267},
  year={2018},
  publisher={Springer}
}

*REF:*
@article{wang2014adaptive,
  title={An adaptive and hierarchical task scheduling scheme for multi-core clusters},
  author={Wang, Yizhuo and Zhang, Yang and Su, Yan and Wang, Xiaojun and Chen, Xu and Ji, Weixing and Shi, Feng},
  journal={Parallel Computing},
  volume={40},
  number={10},
  pages={611--627},
  year={2014},
  publisher={Elsevier}
}

*REF:*
@inproceedings{hansen2014heuristics,
  title={Heuristics for robust allocation of resources to parallel applications with uncertain execution times in heterogeneous systems with uncertain availability},
  author={Hansen, Timothy and Ciorba, Florina M and Maciejewski, Anthony A and Siegel, Howard Jay and Srivastava, Srishti and Banicescu, Ioana},
  booktitle={Proceedings of the World Congress on Engineering},
  volume={1},
  year={2014}
}

*REF:*
@inproceedings{ciorba2012combined,
  title={A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability},
  author={Ciorba, Florina M and Hansen, Timothy and Srivastava, Srishti and Banicescu, Ioana and Maciejewski, Anthony A and Siegel, Howard Jay},
  booktitle={Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW), 2012 IEEE 26th International},
  pages={193--207},
  year={2012},
  organization={IEEE}
}

*REF:*
@inproceedings{pilla2012hierarchical,
  title={A hierarchical approach for load balancing on parallel multi-core systems},
  author={Pilla, Laercio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V},
  booktitle={Parallel Processing (ICPP), 2012 41st International Conference on},
  pages={118--127},
  year={2012},
  organization={IEEE}
}

*REF:*
@article{pasquali2011multi,
  title={A multi-level scheduler for batch jobs on grids},
  author={Pasquali, Marco and Baraglia, Ranieri and Capannini, Gabriele and Ricci, Laura and Laforenza, Domenico},
  journal={The Journal of Supercomputing},
  volume={57},
  number={1},
  pages={81--98},
  year={2011},
  publisher={Springer}
}

*REF:*
@inproceedings{umale2010optimized,
  title={Optimized grid scheduling using two level decision algorithm (TLDA)},
  author={Umale, Jayant and Mahajan, Sunita},
  booktitle={Parallel Distributed and Grid Computing (PDGC), 2010 1st International Conference on},
  pages={78--82},
  year={2010},
  organization={IEEE}
}

*REF:*
@article{blelloch2007multiscale,
  title={Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and in Practice},
  author={Blelloch, Guy E and Blum, Lenore and Harchol-Balter, Mor and Harper, Robert},
  year={2007}
}

*REF:*
@article{kobayashi2001multiscale,
  title={Multiscale computing},
  author={Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
  journal={Proceedings of the National Academy of Sciences},
  volume={98},
  number={22},
  pages={12344--12345},
  year={2001},
  publisher={National Acad Sciences}
}

*REF:*
@article {Kobayashi12344,
	author = {Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
	title = {Multiscale computing},
	volume = {98},
	number = {22},
	pages = {12344--12345},
	year = {2001},
	doi = {10.1073/pnas.231384298},
	publisher = {National Academy of Sciences},
	abstract = {Multiscale computing (MSC) involves the computation, manipulation, and analysis of information at different resolution levels. Widespread use of MSC algorithms and the discovery of important relationships between different approaches to implementation were catalyzed, in part, by the recent interest in wavelets. We present two examples that demonstrate how MSC can help scientists understand complex data. The first is from acoustical signal processing and the second is from computer graphics. MSC,multiscale computing},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/98/22/12344},
	eprint = {http://www.pnas.org/content/98/22/12344.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

*REF:*
@article{huguet2000mixed,
  title={Mixed task scheduling and resource allocation problems},
  author={Huguet, Marie-Jos{\'e} and Lopez, Pierre},
  journal={Proceedings of CP-AI-ORâ€™00, Paderborn, Germany},
  pages={71--79},
  year={2000}
}
* Tools list MLS
** SMB-1
Spark Multi-User Benchmark, SMB, v. 1, 2016
hub.jazz.net/project/pc4spark/SparkMulti-UserBenchmark-1
Measures resource manager performance for Spark (data analysis) workloads in a multi-user scenario
Simulates multiple users submitting short-duration jobs concurrently to systems managed by a resource manager (such as Apache YARN, Apache Mesos, or IBM Platform Conductor for Spark)
** Check!
chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/http://www-mount.ece.umn.edu/~jjyi/MoBS/2009/program/02E-Bienia.pdf
* From Location to Location Pattern Privacy 
h-index?
Cloaking
mapas "camuflados" regioes camufladas. para nao revelar a posicao real da pessoa

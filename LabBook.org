#+TITLE: Labbook Jonas PhD on MLS 
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Jonas(J) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)
* Description of the experimental platform
  + Describe 
  + Describe 

* 2016-04-04 EXAMPLE GRAPHS R
** Just Organizing
Load the data:

#+begin_src R :results output :session :exports both
  df2 <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", sep=" ");
  names(df2) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine");
  df2 <- df2[df2$Disk == "SSD", ];
  df2 <- df2[df2$Scalability == "Strong", ];
  df2 <- df2[df2$Pinned == "Pin", ];
#+end_src

#+RESULTS:

#+begin_src R :results output :session :exports both
summary (df2[df2$Thread == 1,]$Time);
sequential_time = mean(df2[df2$Thread == 1 & df2$Machine == "bali",]$Time);
sequential_time
#+end_src

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
:   484.5   490.3   707.4   644.1   792.0   793.0
: [1] 496.0953

#+begin_src R :results output :session :exports both
library(plyr)
cdata <- ddply(df2[df2$Threads != 1,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               sequential_time = mean(df2[df2$Threads == 1 & df2$Machine == Machine,]$Time),
               speedup_mean = sequential_time/mean(Time),
               speedup_se = 3 * sd(sequential_time/Time) / sqrt(N));
cdata
#+end_src

#+RESULTS:
#+begin_example
  Threads Disk Pinned Machine  N time_mean    time_se sequential_time
1       2  SSD    Pin    bali 30  289.7503 6.05618067        496.0953
2       2  SSD    Pin  turing 30  388.4730 0.39249323        792.0303
3       4  SSD    Pin    bali 30  151.7505 0.50277448        496.0953
4       4  SSD    Pin  turing 30  222.2863 0.07860519        792.0303
5       8  SSD    Pin    bali 30   79.9273 0.22343101        496.0953
6       8  SSD    Pin  turing 30  182.1544 5.69851655        792.0303
7      16  SSD    Pin    bali 30   53.0513 0.15579382        496.0953
8      16  SSD    Pin  turing 30  743.2732 9.97565459        792.0303
  speedup_mean  speedup_se
1     1.712147 0.035278607
2     2.038830 0.002059055
3     3.269150 0.010766602
4     3.563109 0.001260241
5     6.206832 0.017204858
6     4.348126 0.144101927
7     9.351238 0.027443202
8     1.065598 0.015460372
#+end_example

#+begin_src R :results output :session :exports both
s = c(1, 2, 4, 8, 16)
e = c(1, 1, 1, 1, 1)
ideal = data.frame(s, s, s, e)
names(ideal) <- c("P", "Threads", "speedup_mean", "Efficiency")
ideal$Threads <- as.integer(ideal$Threads);
ideal$Pinned <- NA
ideal$Machine <- NA
ideal
#+end_src

#+RESULTS:
:    P Threads speedup_mean Efficiency Pinned Machine
: 1  1       1            1          1     NA      NA
: 2  2       2            2          1     NA      NA
: 3  4       4            4          1     NA      NA
: 4  8       8            8          1     NA      NA
: 5 16      16           16          1     NA      NA

#+begin_src R :results output graphics :file img/speedup-with-variability-jonas-diss.pdf :exports both :width 6 :height 4 :session

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=speedup_mean, color=Machine)) +
      geom_line(data=ideal, aes(group=Machine)) +
      geom_point(size=4, alpha=.5) +
      geom_line(aes(group=Machine)) +
      theme_bw() +
      ylim(0,NA) +
     #xlim(0,NA) +
      scale_x_continuous(breaks=s) +
      geom_errorbar(aes(ymax = speedup_mean+speedup_se, ymin=speedup_mean-speedup_se), width=.5);
p
#+end_src

#+RESULTS:
[[file:img/speedup-with-variability-jonas-diss.pdf]]

** Example
#+begin_src R :results output graphics :file img/finalImgs/strong-speedup-with-variabilityTuring6-jonas-diss.pdf :exports both :width 6 :height 4 :session
df3 <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", sep=" ");
names(df3) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine");
df3 <- df3[df3$Disk == "SSD", ];
df3 <- df3[df3$Scalability == "Strong", ];
df3 <- df3[df3$Pinned == "Free", ];

summary (df3[df3$Thread == 1,]$Time);
sequential_time = mean(df3[df3$Thread == 1 & df3$Machine == "bali",]$Time);
library(plyr);
cdata <- ddply(df3[df3$Threads != 1,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               sequential_time = min(df3[df3$Threads == 1 & df3$Machine == Machine,]$Time),
               SpeedUp = sequential_time/min(Time),
               speedup_se = 3 * sd(sequential_time/Time) / sqrt(N));

s = c(1, 2, 4, 8, 16);
e = c(1, 1, 1, 1, 1);
ideal = data.frame(s, s, s, e);
names(ideal) <- c("P", "Threads", "SpeedUp", "Efficiency");
ideal$Threads <- as.integer(ideal$Threads);
ideal$Pinned <- NA;
ideal$Machine <- NA;

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=SpeedUp, color=Machine)) +
  geom_line(data=ideal, aes(group=Machine), color="black",  alpha=.7) +
  geom_point(size=2, alpha=.9) +
  geom_line(aes(group=Machine)) +
  ggtitle("SpeedUp - Strong Scaling") +
  theme_bw() +
  scale_y_continuous(breaks=s) +
  scale_x_continuous(breaks=s) +
  geom_errorbar(aes(ymax = SpeedUp+speedup_se, ymin=SpeedUp-speedup_se), width=.5);
print(p);

#+end_src

#+RESULTS:
[[file:img/finalImgs/strong-speedup-with-variabilityTuring6-jonas-diss.pdf]]

** Example
#+begin_src R :results output graphics :file img/finalImgs/strong-efficiecy-with-variabilityTuring6-jonas-diss.pdf :exports both :width 6 :height 4 :session
df <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", header=FALSE, sep=" ")
names(df) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine")
df2 <- df[df$Disk == "SSD" & df$Scalability == "Strong" & df$Pinned == "Free" ,]
library(plyr)
cdata <- ddply(df2[df2$Threads != 20,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               seq_time = min(df2[df2$Threads == 1 & df2$Pinned == Pinned & df2$Machine == Machine,]$Time),
               SpeedUp = seq_time/min(Time),
               Efficiency= min(SpeedUp/Threads),
               Efficiency_se = 3 * sd((seq_time/Time)/Threads) / sqrt(N));

print (cdata)
s = c(1, 2, 4, 8, 16)
e = c(1, 1, 1, 1, 1)
sss = c(1, 0.85, 0.7, 0.55, 0.4, 0.25, 0.1)
ideal = data.frame(s, s, e)
names(ideal) <- c("P", "Threads", "Efficiency")
ideal$Threads <- as.integer(ideal$Threads);
ideal$Machine <- NA

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=Efficiency, color=Machine)) +
  geom_line(data=ideal, aes(group=Machine), alpha=.7) +
  geom_point(size=2, alpha=.9) +
  geom_line(aes(group=Machine)) +
  theme_bw() +
  ggtitle("Efficiency - Strong Scaling") +
  scale_x_continuous(breaks=s) +
  scale_y_continuous(breaks=sss) +
  geom_errorbar(aes(ymax = Efficiency+Efficiency_se, ymin=Efficiency-Efficiency_se), width=.5);
print(p);
#+end_src

#+RESULTS:
[[file:img/finalImgs/strong-efficiecy-with-variabilityTuring6-jonas-diss.pdf]]

* 2016-04-04 To export to PDF

Next code block will be exported to =file.png=

#+begin_src R :results output graphics :file file.png :exports both :width 600 :height 400 :session

#+end_src

Next code block will be exported to =file.pdf=. See witdth and height in
incles, not pixels.

#+begin_src R :results output graphics :file file.pdf :exports both :width 6 :height 4 :session

#+end_src
* 2018-05 Papers PhD Brief
** 2018-05-03 Mixed Task Scheduling and Resource Allocation Problems 2000
(a bit confuse article)
The paper presents a constraint-based approach for mixed task
scheduling and resource problem. Two types of constraints: temportal
constrained problem and/or time and resource constrained problem.
 
However, since resource constraints are modelled by temporal
constraints, the semantics of the constraints is forgotten 
and the algorithm cannot consider the specificity of 
TSRA (see section 4).

*REF:*
@article{huguet2000mixed,
  title={Mixed task scheduling and resource allocation problems},
  author={Huguet, Marie-Jos{\'e} and Lopez, Pierre},
  journal={Proceedings of CP-AI-OR’00, Paderborn, Germany},
  pages={71--79},
  year={2000}
}
** 2018-05-03 Multiscale computing (From the Academy) PNAS 2001

Wavelet approach
Multiscale Computer Graphics

Paper objective: describe how MSC can help scientists understand
complex data through two examples: one from acoustical signal
processing and second from computer graphics.

Some phrases:
"the field has undergone tremendous advances during the past decade
because of the increase in inexpensive, powerful hardware." 
"MSC is used in many disciplines, but its presence is often obscured,
because it appears unter several different names depending on the
field of application."

*REF:*
@article {Kobayashi12344,
	author = {Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
	title = {Multiscale computing},
	volume = {98},
	number = {22},
	pages = {12344--12345},
	year = {2001},
	doi = {10.1073/pnas.231384298},
	publisher = {National Academy of Sciences},
	abstract = {Multiscale computing (MSC) involves the computation, manipulation, and analysis of information at different resolution levels. Widespread use of MSC algorithms and the discovery of important relationships between different approaches to implementation were catalyzed, in part, by the recent interest in wavelets. We present two examples that demonstrate how MSC can help scientists understand complex data. The first is from acoustical signal processing and the second is from computer graphics. MSC,multiscale computing},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/98/22/12344},
	eprint = {http://www.pnas.org/content/98/22/12344.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

** 2018-05-07 Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and Practice 2007
Look again
Look again page 6


Some phrases
"A chief characteristic of next-generation computing systems is the
prevalence of parallelism at multiple levels of granularity."page 1 - 1

"the overall goal of the scheduler is to map tasks to processors so
that dependencies in the graph are not violated and execution time
and/or space is minimized." page 2 - 1

"The idea of multiscale scheduling, then, is to integrate cooperative 
and competitive scheduling methods into a unified framework that takes
account of both levels to minimize ERT of competitively scheduled
jobs while permitting their decomposition into cooperatively scheduled
tasks." page 2 - 5



*REF:*
@article{blelloch2007multiscale,
  title={Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and in Practice},
  author={Blelloch, Guy E and Blum, Lenore and Harchol-Balter, Mor and Harper, Robert},
  year={2007}
}
** 2018-05-07 Two level adaptive scheduling JSSPP 2009 - *Not Working*
** 2018-05-07 Optimized Grid Scheduling Using Two Level Decision Algorithm (TLDA) 2010

Combined schedulling starting by ACO (Ant Colony Optimization) and 
then GA (Genetic algoritim)

"TLDA (Two Level Decision Algorithm) shows improvement over nature
based algorithms applied independently"
"The overhead of decision making time can be neglected as 
compared to improvement in execution time"

The work shows that the overhead caused by the decision phase of the
schedulling can be neglected considering the execution time improvement.


*REF:*
@inproceedings{umale2010optimized,
  title={Optimized grid scheduling using two level decision algorithm (TLDA)},
  author={Umale, Jayant and Mahajan, Sunita},
  booktitle={Parallel Distributed and Grid Computing (PDGC), 2010 1st International Conference on},
  pages={78--82},
  year={2010},
  organization={IEEE}
}
** 2018-05-08 Compilers and More: Programming at Exascale - report - 2011
Levels of paralelism
** 2018-05-09 A multi-level scheduler for batch jobs on grids - 2011

*PAPER WITH GOOD STRUCTURE*
  
They proposes a two-level scheduler for dynamically scheduling a
continuous stream of sequential and multi-threaded batch jobs on
grids, "made up of interconnected clusters of heterogeneous
single-processor and/or symmetric multi- processor machines."

"At the top of the hierarchy a lightweight meta-scheduler (MS) clas-
sifies incoming jobs according to their requirements, and schedules them among the
underlying resources balancing the workload. At cluster level a Flexible Backfilling
algorithm carries out the job machine associations by exploiting dynamic informa-
tion about the environment."

"In this paper we describe the study conducted to develop a two-level queue-based
scheduling framework to schedule a continuous stream of independent batch jobs in
grids."

"Moreover, *our scheduler can be classified as static*, this meaning that jobs are as-
signed to the appropriate resources before their execution begins. Once started, they
run on the same resources without interruption."

"The OAR and KOALA queue-based multi-level schedulers are described respec-
tively in 13 and 14. OAR is based on backfilling."

*Meta-schedule.* Defines which job goes to which cluster based on two
functions *Load* and *Ordering*. *Load* aims to dispatch jobs among clusters
considering their workload by assigning a job to the less loaded
cluster. *Ordering* considers the priority of the jobs to balance the number of jobs
with same priority in each cluster queue.

*Local-scheduler.* "Flexible Backfilling algorithm
that selects the machines suitable to perform a job considering the number of proces-
sors and the licenses exploitable on a machine."

"MS Heuristics: MS classifies submitted jobs and dispatches them to LSs. At LS
level, scheduling decision are made by means of a Flexible Backfilling algorithm,
which exploits job priorities computed by MS. Any job prioritization is performed
at LS level. Higher the job priority is, higher the position of the job in LSs’ queues
is."

"The proposed solution aims to schedule arriving jobs balancing the
clusters workload, respecting the job running require-ments 
and deadlines, and optimizing the utilization of hardware and software
resources."

"The conducted simulation tests demonstrated that the investigated
solution can be a viable one. In particular, we show that using a
lightweight component like MS joined with light-ening LSs, carries 
out good results as using more complex LSs."


Published online: 22 February 2011
© Springer Science+Business Media, LLC 2011
*REF:*
@article{pasquali2011multi,
  title={A multi-level scheduler for batch jobs on grids},
  author={Pasquali, Marco and Baraglia, Ranieri and Capannini, Gabriele and Ricci, Laura and Laforenza, Domenico},
  journal={The Journal of Supercomputing},
  volume={57},
  number={1},
  pages={81--98},
  year={2011},
  publisher={Springer}
}
** 2018-05-12 A Hierarchical Approach for Load Balancing on Parallel Multi-core Systems 2012 International Conference on Parallel Processing
"We introduce N UCO LB, a topology-aware load balancer that focuses on
redistributing work while reducing communication costs among and
within compute nodes."

"The NUMA architecture is a scalable solution to alleviate the memory
wall problem, and to provide better scalability for multi-core compute
nodes. A NUMA ar- chitecture features distributed shared memory with
asymmetric memory access costs."

"We introduce the N UCO LB load balancer, which combines information
about the NUMA multi-core topology, the interconnection network
latencies and statistics of the application captured during
execution."

"Thus, our objective for load balancing is to both maximize the use of
the cores (minimize idleness) and also minimize the communication
costs experienced by the application (maximize locality nd affinity)"

"On these systems, an action taken by the load balancer to equalize
the load on the available processors may actually make the overall 
performance worse by increasing the communication time."

"The load balancer needs to know how far from each other the tasks are
mapped, so that it can reduce the communication costs."

"In order to efficiently utilize a parallel machine, a load balancing
algorithm must consider not only the computational load of the
application, but also the existing asymmetries in memory latencies and
bandwidth, and network communication costs."

*REF:*
@inproceedings{pilla2012hierarchical,
  title={A hierarchical approach for load balancing on parallel multi-core systems},
  author={Pilla, Laercio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V},
  booktitle={Parallel Processing (ICPP), 2012 41st International Conference on},
  pages={118--127},
  year={2012},
  organization={IEEE}
}
** 2018-05-15 A Combined Dual-stage Framework for Robust Scheduling of Scientific Applications in Heterogeneous Environments with Uncertain Availability 2012
"A dual-stage framework is proposed in this paper to evaluate the
robustness of efficient resource allocation and dynamic load balancing
of scientific applications in heterogeneous computing environments with uncertain availability."

"The work presented herein demonstrates that using robust resource
allocation (RA) heuristics and application load balancing via
dynamic loop scheduling (DLS) techniques, in concert, will enhance the
execution of computationally intensive scientific applications in
uncertain heterogeneous systems."


"The goal of this research is to assign applications to heterogeneous
computing systems and execute them in such a way that all applications
complete before a common deadline, and their completion times are
robust against uncertainty in input data and system availability."

"Contribution. The main contribution of this paper is the design of an
intelligent two-stage framework to solve the problem of allocating
resources to applications to maximize the probability that the
applications can complete by a common deadline given uncertainty in
the input data and system availability, including developing a
mathematical model of this environment."

*REF:*
@inproceedings{ciorba2012combined,
  title={A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability},
  author={Ciorba, Florina M and Hansen, Timothy and Srivastava, Srishti and Banicescu, Ioana and Maciejewski, Anthony A and Siegel, Howard Jay},
  booktitle={Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW), 2012 IEEE 26th International},
  pages={193--207},
  year={2012},
  organization={IEEE}
}
** 2018-05-15 Heuristics for Robust Allocation of Resources to Parallel Applications with Uncertain Execution Times in Heterogeneous Systems with Uncertain Availability 2014

To allocate resources to applications, we propose a new
batch scheduler. The batch scheduler must allocate resources
in the presence of the two uncertainties of application
execution times and system availability. To minimize the
impact of the two sources of uncertainty on achieving the
makespan goal, our resource allocations should be robust
against these uncertainties.

This paper is based on the first stage of the dual-stage
optimization framework introduced in [10]. In the first stage,
which is the focus of this paper, a batch of applications is
allocated resources from a set of heterogeneous processor
types.

[10] F. M. Ciorba, T. Hansen, S. Srivastava, I. Banicescu, A. A. Ma-
ciejewski, and H. J. Siegel, “A combined dual-stage framework for
robust scheduling of scientific applications in heterogeneous environ-
ments with uncertain availability,” in 21st Heterogeneity in Computing
Workshop (HCW 2012) in the proceedings of the IEEE International
Parallel and Distributed Processing Symposium, May 2012, pp. 193–
207.

*REF:*
@inproceedings{hansen2014heuristics,
  title={Heuristics for robust allocation of resources to parallel applications with uncertain execution times in heterogeneous systems with uncertain availability},
  author={Hansen, Timothy and Ciorba, Florina M and Maciejewski, Anthony A and Siegel, Howard Jay and Srivastava, Srishti and Banicescu, Ioana},
  booktitle={Proceedings of the World Congress on Engineering},
  volume={1},
  year={2014}
}
** 2018-05-15 An adaptive and hierarchical task scheduling scheme for multi-core clusters 2014
This paper introduces an adaptive and hierarchical task scheduling scheme (AHS) for
multi-core clusters, in which work-stealing and work-sharing are adaptively used to
achieve load balancing. However, high inter-node communication
costs hinder work-stealing from being directly performed on distributed memory systems.
AHS addresses this issue with the following techniques: (1) initial partitioning, which
reduces the inter-node task migrations; (2) hierarchical scheduling scheme, which
performs work-stealing inside a node before going across the node boundary and adopts
work-sharing to overlap computation and communication at the inter-node level; and
(3) hierarchical and centralized control for inter-node task migration, which improves
the efficiency of victim selection and termination detection.
We evaluated AHS and existing work-stealing schemes on a 16-nodes multi-core cluster.
Experimental results show that AHS outperforms existing schemes by 11–21.4%, for the
benchmarks studied in this paper.


Today, most existing and new cluster systems are multi-core clusters, which present two levels of parallelism. One is
shared memory parallelism within the cluster node. Another is distributed memory parallelism among the cluster nodes.
How to exploit both shared and distributed memory parallelism is a critical issue to run a large application efficiently on
such systems.


Work-stealing has been proven to be an effective method for task scheduling on shared memory systems, in which all the
worker threads have the same priority and victim is selected randomly. However, work-stealing is inefficient when extended
to distributed memory directly. First, the cost of task transfer between cluster nodes is much higher than between the
multiple cores within a node. Traditional work-stealing is not optimal for decreasing the number of task migrations. Second,
the random victim selection results in useless probing, especially when work is sparse. On distributed memory systems, the
overhead of such probing is not negligible. Third, the thief is idle during work-stealing because of passive stealing. On dis-
tributed memory system, high latency of task migration would make the thief node inefficient.

To address above issues, we propose AHS, an adaptive and hierarchical task scheduling scheme for multi-core clusters.
AHS perceives two levels of hierarchy: cluster nodes and multiple cores on each node.

Traditional work-stealing scheme with random victim selection should not be directly used for distributed memory sys-
tems due to the following two problems. First, random victim selection would result in many times of useless probing when
work is sparse. It would degrade the performance because the cost of probing is not low in distributed memory system. Sec-
ond, a thief node only steals work when it becomes idle. During stealing, there is not useful work running on it. This makes
the thief node inefficient especially when the task migration takes a long time.


*Conclusions*
In this paper, we proposed an adaptive and hierarchical task scheduling scheme (AHS) for multi-core clusters, in which
work-stealing and work-sharing are used together to achieve dynamic load balancing. We describe a practical implementa-
tion of AHS, in which a global scheduler makes an initial partitioning of tasks with respect to the pattern of task parallelism,
and cooperates with local schedulers by message passing. Work-stealing is implemented by the local schedulers to balance
load between worker threads on a cluster node, and work-sharing is used in conjunction with work-stealing to achieve load
balancing between the cluster nodes. We present the theoretical, simulation and experimental studies of our technique. The
results show that work-sharing provides performance benefit and AHS outperforms the existing work-stealing schemes with
real programs. As future work, we would like to test AHS in a large scale context with more cluster nodes and with some
other scientific intensive applications. These tests will allow us to better analyze the behavior of AHS.


*REF:*
@article{wang2014adaptive,
  title={An adaptive and hierarchical task scheduling scheme for multi-core clusters},
  author={Wang, Yizhuo and Zhang, Yang and Su, Yan and Wang, Xiaojun and Chen, Xu and Ji, Weixing and Shi, Feng},
  journal={Parallel Computing},
  volume={40},
  number={10},
  pages={611--627},
  year={2014},
  publisher={Elsevier}
}

** 2018-05-15 Multi-stage resource-aware scheduling for data centers with heterogeneous servers 2018

*REF:*
@article{tran2018multi,
  title={Multi-stage resource-aware scheduling for data centers with heterogeneous servers},
  author={Tran, Tony T and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G and Beck, J Christopher},
  journal={Journal of Scheduling},
  volume={21},
  number={2},
  pages={251--267},
  year={2018},
  publisher={Springer}
}
* Weekly Reports
** 2018-05-17 - 2018-05-27
Studies about schedulers in general;
Reading of more papers, own research/papers from the Proposal;
Setting up my new environment, laptop etc;
Overview about benchmarks, nothing deeply studied yet;
Remembering my Master presentation.

--------
Review slides of performance analysis
Discuss...
** 2018-05-28 - 2018-06-04
Weekly report
This week I worked mainly on the CORAL 2 suit benchmark more specifically over qmcpack. Basically I decided to really start 
the task 1 now because before I was just looking around by the general literature.
** 2018-06-05 - 2018-06-07
The benchmarks suite as NAS and CORAL have a lot of applications.
What should I do, should I study each application? 
Like download it and go inside the code etc or just consider the papers about the applications.
* All BibTeX REFs
*REF:*
@article{tran2018multi,
  title={Multi-stage resource-aware scheduling for data centers with heterogeneous servers},
  author={Tran, Tony T and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G and Beck, J Christopher},
  journal={Journal of Scheduling},
  volume={21},
  number={2},
  pages={251--267},
  year={2018},
  publisher={Springer}
}

*REF:*
@article{wang2014adaptive,
  title={An adaptive and hierarchical task scheduling scheme for multi-core clusters},
  author={Wang, Yizhuo and Zhang, Yang and Su, Yan and Wang, Xiaojun and Chen, Xu and Ji, Weixing and Shi, Feng},
  journal={Parallel Computing},
  volume={40},
  number={10},
  pages={611--627},
  year={2014},
  publisher={Elsevier}
}

*REF:*
@inproceedings{hansen2014heuristics,
  title={Heuristics for robust allocation of resources to parallel applications with uncertain execution times in heterogeneous systems with uncertain availability},
  author={Hansen, Timothy and Ciorba, Florina M and Maciejewski, Anthony A and Siegel, Howard Jay and Srivastava, Srishti and Banicescu, Ioana},
  booktitle={Proceedings of the World Congress on Engineering},
  volume={1},
  year={2014}
}

*REF:*
@inproceedings{ciorba2012combined,
  title={A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability},
  author={Ciorba, Florina M and Hansen, Timothy and Srivastava, Srishti and Banicescu, Ioana and Maciejewski, Anthony A and Siegel, Howard Jay},
  booktitle={Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW), 2012 IEEE 26th International},
  pages={193--207},
  year={2012},
  organization={IEEE}
}

*REF:*
@inproceedings{pilla2012hierarchical,
  title={A hierarchical approach for load balancing on parallel multi-core systems},
  author={Pilla, Laercio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V},
  booktitle={Parallel Processing (ICPP), 2012 41st International Conference on},
  pages={118--127},
  year={2012},
  organization={IEEE}
}

*REF:*
@article{pasquali2011multi,
  title={A multi-level scheduler for batch jobs on grids},
  author={Pasquali, Marco and Baraglia, Ranieri and Capannini, Gabriele and Ricci, Laura and Laforenza, Domenico},
  journal={The Journal of Supercomputing},
  volume={57},
  number={1},
  pages={81--98},
  year={2011},
  publisher={Springer}
}

*REF:*
@inproceedings{umale2010optimized,
  title={Optimized grid scheduling using two level decision algorithm (TLDA)},
  author={Umale, Jayant and Mahajan, Sunita},
  booktitle={Parallel Distributed and Grid Computing (PDGC), 2010 1st International Conference on},
  pages={78--82},
  year={2010},
  organization={IEEE}
}

*REF:*
@article{blelloch2007multiscale,
  title={Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and in Practice},
  author={Blelloch, Guy E and Blum, Lenore and Harchol-Balter, Mor and Harper, Robert},
  year={2007}
}

*REF:*
@article{kobayashi2001multiscale,
  title={Multiscale computing},
  author={Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
  journal={Proceedings of the National Academy of Sciences},
  volume={98},
  number={22},
  pages={12344--12345},
  year={2001},
  publisher={National Acad Sciences}
}

*REF:*
@article {Kobayashi12344,
	author = {Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
	title = {Multiscale computing},
	volume = {98},
	number = {22},
	pages = {12344--12345},
	year = {2001},
	doi = {10.1073/pnas.231384298},
	publisher = {National Academy of Sciences},
	abstract = {Multiscale computing (MSC) involves the computation, manipulation, and analysis of information at different resolution levels. Widespread use of MSC algorithms and the discovery of important relationships between different approaches to implementation were catalyzed, in part, by the recent interest in wavelets. We present two examples that demonstrate how MSC can help scientists understand complex data. The first is from acoustical signal processing and the second is from computer graphics. MSC,multiscale computing},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/98/22/12344},
	eprint = {http://www.pnas.org/content/98/22/12344.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

*REF:*
@article{huguet2000mixed,
  title={Mixed task scheduling and resource allocation problems},
  author={Huguet, Marie-Jos{\'e} and Lopez, Pierre},
  journal={Proceedings of CP-AI-OR’00, Paderborn, Germany},
  pages={71--79},
  year={2000}
}
* Tools list MLS
** SMB-1
Spark Multi-User Benchmark, SMB, v. 1, 2016
hub.jazz.net/project/pc4spark/SparkMulti-UserBenchmark-1
Measures resource manager performance for Spark (data analysis) workloads in a multi-user scenario
Simulates multiple users submitting short-duration jobs concurrently to systems managed by a resource manager (such as Apache YARN, Apache Mesos, or IBM Platform Conductor for Spark)
** The Hardware Accelerated Cosmology Code (HACC)
The code is hybrid MPI-OpenMP and depends on external FFT library

** Check!
chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/http://www-mount.ece.umn.edu/~jjyi/MoBS/2009/program/02E-Bienia.pdf

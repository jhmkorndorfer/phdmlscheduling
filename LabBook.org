#+TITLE: Labbook Jonas PhD on MLS Project
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Jonas(J) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)
* IMPORTANT NOTES
** Example: A lot of cores are already available (left side of the figure), however the whole 'job 1542' only ends after that, preventing 'job 1543' to start. If the schedulers communicate between the different levels, maybe they can make better decisions.
[[file:~/Pictures/Screenshot%20from%202018-07-05%2015-52-54.png][Graph Vampir Timeline]]
* Description of the experimental platform
  + Describe 
  + Describe 

* 2016-04-04 EXAMPLE GRAPHS R
** Just Organizing
Load the data:

#+begin_src R :results output :session :exports both
  df2 <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", sep=" ");
  names(df2) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine");
  df2 <- df2[df2$Disk == "SSD", ];
  df2 <- df2[df2$Scalability == "Strong", ];
  df2 <- df2[df2$Pinned == "Pin", ];
#+end_src

#+RESULTS:

#+begin_src R :results output :session :exports both
summary (df2[df2$Thread == 1,]$Time);
sequential_time = mean(df2[df2$Thread == 1 & df2$Machine == "bali",]$Time);
sequential_time
#+end_src

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
:   484.5   490.3   707.4   644.1   792.0   793.0
: [1] 496.0953

#+begin_src R :results output :session :exports both
library(plyr)
cdata <- ddply(df2[df2$Threads != 1,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               sequential_time = mean(df2[df2$Threads == 1 & df2$Machine == Machine,]$Time),
               speedup_mean = sequential_time/mean(Time),
               speedup_se = 3 * sd(sequential_time/Time) / sqrt(N));
cdata
#+end_src

#+RESULTS:
#+begin_example
  Threads Disk Pinned Machine  N time_mean    time_se sequential_time
1       2  SSD    Pin    bali 30  289.7503 6.05618067        496.0953
2       2  SSD    Pin  turing 30  388.4730 0.39249323        792.0303
3       4  SSD    Pin    bali 30  151.7505 0.50277448        496.0953
4       4  SSD    Pin  turing 30  222.2863 0.07860519        792.0303
5       8  SSD    Pin    bali 30   79.9273 0.22343101        496.0953
6       8  SSD    Pin  turing 30  182.1544 5.69851655        792.0303
7      16  SSD    Pin    bali 30   53.0513 0.15579382        496.0953
8      16  SSD    Pin  turing 30  743.2732 9.97565459        792.0303
  speedup_mean  speedup_se
1     1.712147 0.035278607
2     2.038830 0.002059055
3     3.269150 0.010766602
4     3.563109 0.001260241
5     6.206832 0.017204858
6     4.348126 0.144101927
7     9.351238 0.027443202
8     1.065598 0.015460372
#+end_example

#+begin_src R :results output :session :exports both
s = c(1, 2, 4, 8, 16)
e = c(1, 1, 1, 1, 1)
ideal = data.frame(s, s, s, e)
names(ideal) <- c("P", "Threads", "speedup_mean", "Efficiency")
ideal$Threads <- as.integer(ideal$Threads);
ideal$Pinned <- NA
ideal$Machine <- NA
ideal
#+end_src

#+RESULTS:
:    P Threads speedup_mean Efficiency Pinned Machine
: 1  1       1            1          1     NA      NA
: 2  2       2            2          1     NA      NA
: 3  4       4            4          1     NA      NA
: 4  8       8            8          1     NA      NA
: 5 16      16           16          1     NA      NA

#+begin_src R :results output graphics :file img/speedup-with-variability-jonas-diss.pdf :exports both :width 6 :height 4 :session

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=speedup_mean, color=Machine)) +
      geom_line(data=ideal, aes(group=Machine)) +
      geom_point(size=4, alpha=.5) +
      geom_line(aes(group=Machine)) +
      theme_bw() +
      ylim(0,NA) +
     #xlim(0,NA) +
      scale_x_continuous(breaks=s) +
      geom_errorbar(aes(ymax = speedup_mean+speedup_se, ymin=speedup_mean-speedup_se), width=.5);
p
#+end_src

#+RESULTS:
[[file:img/speedup-with-variability-jonas-diss.pdf]]

** Example
#+begin_src R :results output graphics :file img/finalImgs/strong-speedup-with-variabilityTuring6-jonas-diss.pdf :exports both :width 6 :height 4 :session
df3 <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", sep=" ");
names(df3) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine");
df3 <- df3[df3$Disk == "SSD", ];
df3 <- df3[df3$Scalability == "Strong", ];
df3 <- df3[df3$Pinned == "Free", ];

summary (df3[df3$Thread == 1,]$Time);
sequential_time = mean(df3[df3$Thread == 1 & df3$Machine == "bali",]$Time);
library(plyr);
cdata <- ddply(df3[df3$Threads != 1,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               sequential_time = min(df3[df3$Threads == 1 & df3$Machine == Machine,]$Time),
               SpeedUp = sequential_time/min(Time),
               speedup_se = 3 * sd(sequential_time/Time) / sqrt(N));

s = c(1, 2, 4, 8, 16);
e = c(1, 1, 1, 1, 1);
ideal = data.frame(s, s, s, e);
names(ideal) <- c("P", "Threads", "SpeedUp", "Efficiency");
ideal$Threads <- as.integer(ideal$Threads);
ideal$Pinned <- NA;
ideal$Machine <- NA;

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=SpeedUp, color=Machine)) +
  geom_line(data=ideal, aes(group=Machine), color="black",  alpha=.7) +
  geom_point(size=2, alpha=.9) +
  geom_line(aes(group=Machine)) +
  ggtitle("SpeedUp - Strong Scaling") +
  theme_bw() +
  scale_y_continuous(breaks=s) +
  scale_x_continuous(breaks=s) +
  geom_errorbar(aes(ymax = SpeedUp+speedup_se, ymin=SpeedUp-speedup_se), width=.5);
print(p);

#+end_src

#+RESULTS:
[[file:img/finalImgs/strong-speedup-with-variabilityTuring6-jonas-diss.pdf]]

** Example
#+begin_src R :results output graphics :file img/finalImgs/strong-efficiecy-with-variabilityTuring6-jonas-diss.pdf :exports both :width 6 :height 4 :session
df <- read.csv("strong_scalability_data_Dissertation/allTogether_with_Turing_2.csv", header=FALSE, sep=" ")
names(df) <- c("Time", "Threads", "Disk", "Pinned", "Scalability", "Machine")
df2 <- df[df$Disk == "SSD" & df$Scalability == "Strong" & df$Pinned == "Free" ,]
library(plyr)
cdata <- ddply(df2[df2$Threads != 20,], .(Threads, Disk, Pinned, Machine), summarise,
               N = length(Time),
               time_mean = mean(Time),
               time_se = 3 * sd(Time) / sqrt(N),
               seq_time = min(df2[df2$Threads == 1 & df2$Pinned == Pinned & df2$Machine == Machine,]$Time),
               SpeedUp = seq_time/min(Time),
               Efficiency= min(SpeedUp/Threads),
               Efficiency_se = 3 * sd((seq_time/Time)/Threads) / sqrt(N));

print (cdata)
s = c(1, 2, 4, 8, 16)
e = c(1, 1, 1, 1, 1)
sss = c(1, 0.85, 0.7, 0.55, 0.4, 0.25, 0.1)
ideal = data.frame(s, s, e)
names(ideal) <- c("P", "Threads", "Efficiency")
ideal$Threads <- as.integer(ideal$Threads);
ideal$Machine <- NA

library(ggplot2)
p <- ggplot(cdata, aes(x=Threads, y=Efficiency, color=Machine)) +
  geom_line(data=ideal, aes(group=Machine), alpha=.7) +
  geom_point(size=2, alpha=.9) +
  geom_line(aes(group=Machine)) +
  theme_bw() +
  ggtitle("Efficiency - Strong Scaling") +
  scale_x_continuous(breaks=s) +
  scale_y_continuous(breaks=sss) +
  geom_errorbar(aes(ymax = Efficiency+Efficiency_se, ymin=Efficiency-Efficiency_se), width=.5);
print(p);
#+end_src

#+RESULTS:
[[file:img/finalImgs/strong-efficiecy-with-variabilityTuring6-jonas-diss.pdf]]

* 2016-04-04 To export to PDF

Next code block will be exported to =file.png=

#+begin_src R :results output graphics :file file.png :exports both :width 600 :height 400 :session

#+end_src

Next code block will be exported to =file.pdf=. See witdth and height in
incles, not pixels.

#+begin_src R :results output graphics :file file.pdf :exports both :width 6 :height 4 :session

#+end_src
* 2018-05 Papers PhD Brief
** 2018-05-03 Mixed Task Scheduling and Resource Allocation Problems 2000
(a bit confuse article)
The paper presents a constraint-based approach for mixed task
scheduling and resource problem. Two types of constraints: temportal
constrained problem and/or time and resource constrained problem.
 
However, since resource constraints are modelled by temporal
constraints, the semantics of the constraints is forgotten 
and the algorithm cannot consider the specificity of 
TSRA (see section 4).

*REF:*
@article{huguet2000mixed,
  title={Mixed task scheduling and resource allocation problems},
  author={Huguet, Marie-Jos{\'e} and Lopez, Pierre},
  journal={Proceedings of CP-AI-OR’00, Paderborn, Germany},
  pages={71--79},
  year={2000}
}
** 2018-05-03 Multiscale computing (From the Academy) PNAS 2001

Wavelet approach
Multiscale Computer Graphics

Paper objective: describe how MSC can help scientists understand
complex data through two examples: one from acoustical signal
processing and second from computer graphics.

Some phrases:
"the field has undergone tremendous advances during the past decade
because of the increase in inexpensive, powerful hardware." 
"MSC is used in many disciplines, but its presence is often obscured,
because it appears unter several different names depending on the
field of application."

*REF:*
@article {Kobayashi12344,
	author = {Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
	title = {Multiscale computing},
	volume = {98},
	number = {22},
	pages = {12344--12345},
	year = {2001},
	doi = {10.1073/pnas.231384298},
	publisher = {National Academy of Sciences},
	abstract = {Multiscale computing (MSC) involves the computation, manipulation, and analysis of information at different resolution levels. Widespread use of MSC algorithms and the discovery of important relationships between different approaches to implementation were catalyzed, in part, by the recent interest in wavelets. We present two examples that demonstrate how MSC can help scientists understand complex data. The first is from acoustical signal processing and the second is from computer graphics. MSC,multiscale computing},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/98/22/12344},
	eprint = {http://www.pnas.org/content/98/22/12344.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

** 2018-05-07 Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and Practice 2007
Look again
Look again page 6


Some phrases
"A chief characteristic of next-generation computing systems is the
prevalence of parallelism at multiple levels of granularity."page 1 - 1

"the overall goal of the scheduler is to map tasks to processors so
that dependencies in the graph are not violated and execution time
and/or space is minimized." page 2 - 1

"The idea of multiscale scheduling, then, is to integrate cooperative 
and competitive scheduling methods into a unified framework that takes
account of both levels to minimize ERT of competitively scheduled
jobs while permitting their decomposition into cooperatively scheduled
tasks." page 2 - 5



*REF:*
@article{blelloch2007multiscale,
  title={Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and in Practice},
  author={Blelloch, Guy E and Blum, Lenore and Harchol-Balter, Mor and Harper, Robert},
  year={2007}
}
** 2018-05-07 Two level adaptive scheduling JSSPP 2009 - *Not Working*
** 2018-05-07 Optimized Grid Scheduling Using Two Level Decision Algorithm (TLDA) 2010

Combined schedulling starting by ACO (Ant Colony Optimization) and 
then GA (Genetic algoritim)

"TLDA (Two Level Decision Algorithm) shows improvement over nature
based algorithms applied independently"
"The overhead of decision making time can be neglected as 
compared to improvement in execution time"

The work shows that the overhead caused by the decision phase of the
schedulling can be neglected considering the execution time improvement.


*REF:*
@inproceedings{umale2010optimized,
  title={Optimized grid scheduling using two level decision algorithm (TLDA)},
  author={Umale, Jayant and Mahajan, Sunita},
  booktitle={Parallel Distributed and Grid Computing (PDGC), 2010 1st International Conference on},
  pages={78--82},
  year={2010},
  organization={IEEE}
}
** 2018-05-08 Compilers and More: Programming at Exascale - report - 2011
Levels of paralelism
** 2018-05-09 A multi-level scheduler for batch jobs on grids - 2011

*PAPER WITH GOOD STRUCTURE*
  
They proposes a two-level scheduler for dynamically scheduling a
continuous stream of sequential and multi-threaded batch jobs on
grids, "made up of interconnected clusters of heterogeneous
single-processor and/or symmetric multi- processor machines."

"At the top of the hierarchy a lightweight meta-scheduler (MS) clas-
sifies incoming jobs according to their requirements, and schedules them among the
underlying resources balancing the workload. At cluster level a Flexible Backfilling
algorithm carries out the job machine associations by exploiting dynamic informa-
tion about the environment."

"In this paper we describe the study conducted to develop a two-level queue-based
scheduling framework to schedule a continuous stream of independent batch jobs in
grids."

"Moreover, *our scheduler can be classified as static*, this meaning that jobs are as-
signed to the appropriate resources before their execution begins. Once started, they
run on the same resources without interruption."

"The OAR and KOALA queue-based multi-level schedulers are described respec-
tively in 13 and 14. OAR is based on backfilling."

*Meta-schedule.* Defines which job goes to which cluster based on two
functions *Load* and *Ordering*. *Load* aims to dispatch jobs among clusters
considering their workload by assigning a job to the less loaded
cluster. *Ordering* considers the priority of the jobs to balance the number of jobs
with same priority in each cluster queue.

*Local-scheduler.* "Flexible Backfilling algorithm
that selects the machines suitable to perform a job considering the number of proces-
sors and the licenses exploitable on a machine."

"MS Heuristics: MS classifies submitted jobs and dispatches them to LSs. At LS
level, scheduling decision are made by means of a Flexible Backfilling algorithm,
which exploits job priorities computed by MS. Any job prioritization is performed
at LS level. Higher the job priority is, higher the position of the job in LSs’ queues
is."

"The proposed solution aims to schedule arriving jobs balancing the
clusters workload, respecting the job running require-ments 
and deadlines, and optimizing the utilization of hardware and software
resources."

"The conducted simulation tests demonstrated that the investigated
solution can be a viable one. In particular, we show that using a
lightweight component like MS joined with light-ening LSs, carries 
out good results as using more complex LSs."


Published online: 22 February 2011
© Springer Science+Business Media, LLC 2011
*REF:*
@article{pasquali2011multi,
  title={A multi-level scheduler for batch jobs on grids},
  author={Pasquali, Marco and Baraglia, Ranieri and Capannini, Gabriele and Ricci, Laura and Laforenza, Domenico},
  journal={The Journal of Supercomputing},
  volume={57},
  number={1},
  pages={81--98},
  year={2011},
  publisher={Springer}
}
** 2018-05-12 A Hierarchical Approach for Load Balancing on Parallel Multi-core Systems 2012 International Conference on Parallel Processing
"We introduce N UCO LB, a topology-aware load balancer that focuses on
redistributing work while reducing communication costs among and
within compute nodes."

"The NUMA architecture is a scalable solution to alleviate the memory
wall problem, and to provide better scalability for multi-core compute
nodes. A NUMA ar- chitecture features distributed shared memory with
asymmetric memory access costs."

"We introduce the N UCO LB load balancer, which combines information
about the NUMA multi-core topology, the interconnection network
latencies and statistics of the application captured during
execution."

"Thus, our objective for load balancing is to both maximize the use of
the cores (minimize idleness) and also minimize the communication
costs experienced by the application (maximize locality nd affinity)"

"On these systems, an action taken by the load balancer to equalize
the load on the available processors may actually make the overall 
performance worse by increasing the communication time."

"The load balancer needs to know how far from each other the tasks are
mapped, so that it can reduce the communication costs."

"In order to efficiently utilize a parallel machine, a load balancing
algorithm must consider not only the computational load of the
application, but also the existing asymmetries in memory latencies and
bandwidth, and network communication costs."

*REF:*
@inproceedings{pilla2012hierarchical,
  title={A hierarchical approach for load balancing on parallel multi-core systems},
  author={Pilla, Laercio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V},
  booktitle={Parallel Processing (ICPP), 2012 41st International Conference on},
  pages={118--127},
  year={2012},
  organization={IEEE}
}
** 2018-05-15 A Combined Dual-stage Framework for Robust Scheduling of Scientific Applications in Heterogeneous Environments with Uncertain Availability 2012
"A dual-stage framework is proposed in this paper to evaluate the
robustness of efficient resource allocation and dynamic load balancing
of scientific applications in heterogeneous computing environments with uncertain availability."

"The work presented herein demonstrates that using robust resource
allocation (RA) heuristics and application load balancing via
dynamic loop scheduling (DLS) techniques, in concert, will enhance the
execution of computationally intensive scientific applications in
uncertain heterogeneous systems."


"The goal of this research is to assign applications to heterogeneous
computing systems and execute them in such a way that all applications
complete before a common deadline, and their completion times are
robust against uncertainty in input data and system availability."

"Contribution. The main contribution of this paper is the design of an
intelligent two-stage framework to solve the problem of allocating
resources to applications to maximize the probability that the
applications can complete by a common deadline given uncertainty in
the input data and system availability, including developing a
mathematical model of this environment."

*REF:*
@inproceedings{ciorba2012combined,
  title={A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability},
  author={Ciorba, Florina M and Hansen, Timothy and Srivastava, Srishti and Banicescu, Ioana and Maciejewski, Anthony A and Siegel, Howard Jay},
  booktitle={Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW), 2012 IEEE 26th International},
  pages={193--207},
  year={2012},
  organization={IEEE}
}
** 2018-05-15 Heuristics for Robust Allocation of Resources to Parallel Applications with Uncertain Execution Times in Heterogeneous Systems with Uncertain Availability 2014

To allocate resources to applications, we propose a new
batch scheduler. The batch scheduler must allocate resources
in the presence of the two uncertainties of application
execution times and system availability. To minimize the
impact of the two sources of uncertainty on achieving the
makespan goal, our resource allocations should be robust
against these uncertainties.

This paper is based on the first stage of the dual-stage
optimization framework introduced in [10]. In the first stage,
which is the focus of this paper, a batch of applications is
allocated resources from a set of heterogeneous processor
types.

[10] F. M. Ciorba, T. Hansen, S. Srivastava, I. Banicescu, A. A. Ma-
ciejewski, and H. J. Siegel, “A combined dual-stage framework for
robust scheduling of scientific applications in heterogeneous environ-
ments with uncertain availability,” in 21st Heterogeneity in Computing
Workshop (HCW 2012) in the proceedings of the IEEE International
Parallel and Distributed Processing Symposium, May 2012, pp. 193–
207.

*REF:*
@inproceedings{hansen2014heuristics,
  title={Heuristics for robust allocation of resources to parallel applications with uncertain execution times in heterogeneous systems with uncertain availability},
  author={Hansen, Timothy and Ciorba, Florina M and Maciejewski, Anthony A and Siegel, Howard Jay and Srivastava, Srishti and Banicescu, Ioana},
  booktitle={Proceedings of the World Congress on Engineering},
  volume={1},
  year={2014}
}
** 2018-05-15 An adaptive and hierarchical task scheduling scheme for multi-core clusters 2014
This paper introduces an adaptive and hierarchical task scheduling scheme (AHS) for
multi-core clusters, in which work-stealing and work-sharing are adaptively used to
achieve load balancing. However, high inter-node communication
costs hinder work-stealing from being directly performed on distributed memory systems.
AHS addresses this issue with the following techniques: (1) initial partitioning, which
reduces the inter-node task migrations; (2) hierarchical scheduling scheme, which
performs work-stealing inside a node before going across the node boundary and adopts
work-sharing to overlap computation and communication at the inter-node level; and
(3) hierarchical and centralized control for inter-node task migration, which improves
the efficiency of victim selection and termination detection.
We evaluated AHS and existing work-stealing schemes on a 16-nodes multi-core cluster.
Experimental results show that AHS outperforms existing schemes by 11–21.4%, for the
benchmarks studied in this paper.


Today, most existing and new cluster systems are multi-core clusters, which present two levels of parallelism. One is
shared memory parallelism within the cluster node. Another is distributed memory parallelism among the cluster nodes.
How to exploit both shared and distributed memory parallelism is a critical issue to run a large application efficiently on
such systems.


Work-stealing has been proven to be an effective method for task scheduling on shared memory systems, in which all the
worker threads have the same priority and victim is selected randomly. However, work-stealing is inefficient when extended
to distributed memory directly. First, the cost of task transfer between cluster nodes is much higher than between the
multiple cores within a node. Traditional work-stealing is not optimal for decreasing the number of task migrations. Second,
the random victim selection results in useless probing, especially when work is sparse. On distributed memory systems, the
overhead of such probing is not negligible. Third, the thief is idle during work-stealing because of passive stealing. On dis-
tributed memory system, high latency of task migration would make the thief node inefficient.

To address above issues, we propose AHS, an adaptive and hierarchical task scheduling scheme for multi-core clusters.
AHS perceives two levels of hierarchy: cluster nodes and multiple cores on each node.

Traditional work-stealing scheme with random victim selection should not be directly used for distributed memory sys-
tems due to the following two problems. First, random victim selection would result in many times of useless probing when
work is sparse. It would degrade the performance because the cost of probing is not low in distributed memory system. Sec-
ond, a thief node only steals work when it becomes idle. During stealing, there is not useful work running on it. This makes
the thief node inefficient especially when the task migration takes a long time.


*Conclusions*
In this paper, we proposed an adaptive and hierarchical task scheduling scheme (AHS) for multi-core clusters, in which
work-stealing and work-sharing are used together to achieve dynamic load balancing. We describe a practical implementa-
tion of AHS, in which a global scheduler makes an initial partitioning of tasks with respect to the pattern of task parallelism,
and cooperates with local schedulers by message passing. Work-stealing is implemented by the local schedulers to balance
load between worker threads on a cluster node, and work-sharing is used in conjunction with work-stealing to achieve load
balancing between the cluster nodes. We present the theoretical, simulation and experimental studies of our technique. The
results show that work-sharing provides performance benefit and AHS outperforms the existing work-stealing schemes with
real programs. As future work, we would like to test AHS in a large scale context with more cluster nodes and with some
other scientific intensive applications. These tests will allow us to better analyze the behavior of AHS.


*REF:*
@article{wang2014adaptive,
  title={An adaptive and hierarchical task scheduling scheme for multi-core clusters},
  author={Wang, Yizhuo and Zhang, Yang and Su, Yan and Wang, Xiaojun and Chen, Xu and Ji, Weixing and Shi, Feng},
  journal={Parallel Computing},
  volume={40},
  number={10},
  pages={611--627},
  year={2014},
  publisher={Elsevier}
}

** 2018-05-15 Multi-stage resource-aware scheduling for data centers with heterogeneous servers 2018

This paper presents a three-stage algorithm for
resource-aware scheduling of computational jobs in a large-
scale heterogeneous data center. The algorithm aims to
allocate job classes to machine configurations to attain an
efficient mapping between job resource request profiles and
machine resource capacity profiles. The first stage uses a
queueing model that treats the system in an aggregated man-
ner with pooled machines and jobs represented as a fluid
flow. The latter two stages use combinatorial optimization
techniques to solve a shorter-term, more accurate represen-
tation of the problem using the first-stage, long-term solution
for heuristic guidance.

We present experimental results of our algorithm
on both Google workload trace data and generated data and
show that it outperforms existing schedulers. These results
illustrate the importance of considering heterogeneity of both
job and machine configuration profiles in making effective
scheduling decisions.

*REF:*
@article{tran2018multi,
  title={Multi-stage resource-aware scheduling for data centers with heterogeneous servers},
  author={Tran, Tony T and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G and Beck, J Christopher},
  journal={Journal of Scheduling},
  volume={21},
  number={2},
  pages={251--267},
  year={2018},
  publisher={Springer}
}
** 2018-06-08 A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability 2012
Scheduling parallel applications on ex-
isting or emerging computing platforms is challeng-
ing, and, among other attributes, must be efficient
and robust

Scientific applications express the solutions to
complex scientific problems, which often are data-
parallel and contain large loops. The execution
of such applications in heterogeneous computing
environments is computationally intensive and ex-
hibits an irregular behavior, in general due to
variations of algorithmic and systemic nature [1,
ch. 4]. Distribution of input data and variations
of algorithmic nature cause intrinsic imbalance,
while variations of systemic nature cause extrinsic
imbalance [2]. Load imbalance in computationally
intensive scientific applications is often their ma-
jor performance degradation factor [1][2]. Tradi-
tionally, solutions that address load imbalance in
scientific applications involve dynamic data and/or
work re-distribution.

The work presented herein demonstrates that
using robust resource allocation (RA) heuristics [3]
and application load balancing via dynamic loop
scheduling (DLS) techniques, in concert, will en-
hance the execution of computationally intensive
scientific applications in uncertain heterogeneous
systems.The goal of this research is to assign
applications to heterogeneous computing systems
and execute them in such a way that all applications
complete before a common deadline, and their
completion times are robust against uncertainty in
input data and system availability.

To accomplish this goal, the approach proposed
herein is to divide the execution of scientific appli-
cations on heterogeneous computing systems into
two stages, as outlined in Figure 1:

*Stage I* initial mapping–resources are allocated to
each application according to a given robust RA
policy.

*Stage II* runtime application scheduling–the execu-
tion of each application is optimized, for the set of
resources allocated in the previous stage, according
to a given robust application scheduling strategy.
Initial mapping (IM) can be defined as the prob-
lem of finding a mapping of a batch of applications
onto a set of resources to maximize robustness
against uncertain input data and system availabil-
ity. Robustness here is defined as the probability
that applications are completed on the allocated
resources by a common deadline [4].

*Motivation for Stage I*. The motivation for solv-
ing the IM problem via robust RA is to avoid the
runtime resource reallocation problem, i.e., reallo-
cating resources already assigned to applications to
avoid violations of the performance objective. The
robustness of an RA can be quantified as the joint
probability that all applications will complete by
their deadline given the uncertain input data and
system availability.

*Motivation for Stage II*. Just as in stage I, un-
certain runtime availability of resources allocated
to an application, as well as uncertain input data,
are known sources of uncertainty in stage II and 
may impact the applications execution times. The
motivation for this stage is based on the assump-
tion that a specific runtime application scheduling
(RAS) policy exists that avoids the runtime re-
source reallocation problem and that satisfies the
stated performance objective, while possibly allow-
ing a larger degree of uncertainty in input data and
system availability.

*Usefulness.* The usefulness of the proposed
combined dual-stage framework is based on the
following hypothesis: using an intelligent approach
in both stages will result in better overall system
performance than using an intelligent approach
for either stage in isolation or neither. The dual-
stage framework allows investigation of the over-
all degree of tolerable uncertainty, such that the
desired performance objective is satisfied, for each
application individually and the entire collection of
applications running on the heterogeneous comput-
ing system.

*Contribution.* The main contribution of this pa-
per is the design of an intelligent two-stage frame-
work to solve the problem of allocating resources
to applications to maximize the probability that the
applications can complete by a common deadline
given uncertainty in the input data and system
availability, including developing a mathematical
model of this environment.

*Makespan*
If we let Shmuel feed all goats, then the makespan is 30 (3×10 for Shmuel, 0 for Shifra);
If we let Shifra feed one goat and Shmuel two goats, then the makespan is 20 (2×10 for Shmuel, 12 for Shifra);
If we let Shifra feed two goats and Shmuel one goat, then the makespan is 24 (2×12 for Shifra, 10 for Shmuel);
If we let Shifra feed all goats, then the makespan is 36 (3×12 for Shifra).


*REF*
@INPROCEEDINGS{CombinedDualstageFrameworkScheduling, 
author={F. M. Ciorba and T. Hansen and S. Srivastava and I. Banicescu and A. A. Maciejewski and H. J. Siegel}, 
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum}, 
title={A Combined Dual-stage Framework for Robust Scheduling of Scientific Applications in Heterogeneous Environments with Uncertain Availability}, 
year={2012}, 
volume={}, 
number={}, 
pages={193-207}, 
keywords={natural sciences computing;parallel processing;probability;resource allocation;scheduling;combined dual-stage framework;dynamic load balancing;heterogeneous computing environments;parallel application scheduling;probability maximization;robust dynamic loop scheduling techniques;robust resource allocation heuristics;scientific applications;system make span minimization;uncertain availability;Availability;Dynamic scheduling;Program processors;Resource management;Robustness;Runtime;Uncertainty;dynamic loop scheduling;heterogeneous systems;high performance;non-dedicated systems;resource allocation;robustness;uncertainties}, 
doi={10.1109/IPDPSW.2012.5}, 
ISSN={}, 
month={May},}
** 2018-07-05 Exploring the Relation Between Two Levels ofScheduling Using a Novel Simulation Approach 2017
The present work explores the relation between two scheduling levels: batch and application. To understand and explore this relation, a novel simulation approach is presented
that bridges two existing simulators from the two scheduling levels. A novel two-level simulator that implements the proposed
approach is introduced. The two-level simulator is used to simulate all combinations of three batch scheduling and four
application scheduling algorithms from the literature. These combinations are considered for allocating resources and executing
the parallel jobs from a workload of a production HPC system.

As a preliminary step for the work in the present paper,the original Alea simulator [12] has beenredesignedandreimplementedto support ALS algorithms in addition to BLSalgorithms, in [3]. 

Example: A lot of cores are already available (left side of the figure), however the whole 'job 1542' only ends after that, preventing 'job 1543' to start. If the schedulers communicate between the different levels, maybe they can make better decisions. 
[[file:~/Pictures/Screenshot%20from%202018-07-05%2015-52-54.png][Graph Vampir TimeLine]]

*REF*
@INPROCEEDINGS{TwolevelSchedulingAhmed, 
author={A. Eleliemy and A. Mohammed and F. M. Ciorba}, 
booktitle={2017 16th International Symposium on Parallel and Distributed Computing (ISPDC)}, 
title={Exploring the Relation between Two Levels of Scheduling Using a Novel Simulation Approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={26-33}, 
keywords={parallel processing;scheduling;application scheduling algorithms;batch scheduling;modern high performance computing systems;novel simulation approach;respective level;scheduling levels;two-level simulator;Computational modeling;Hardware;Scheduling;Scheduling algorithms;Alea;Application level scheduling;Batch level scheduling;GridSim;High performance computing;OTF2;SimDag;SimGrid;Two-level scheduling;Vampir.}, 
doi={10.1109/ISPDC.2017.23}, 
ISSN={}, 
month={July},}

** 2018-07-06 Efficient Generation of Parallel Spin-images UsingDynamic Loop Scheduling 2017
 Thiswork  introduces  an  efficient  version  of  the  parallel  spin-image
algorithm  (PSIA),  called  EPSIA.  The  PSIA  is  a  parallel  version
of   the   spin-image   algorithm   (SIA).   The   (P)SIA   is   used   in
various  domains,  such  as  3D  object  recognition,  categorization,
and  3D  face  recognition.  EPSIA  refers  to  the  extended  version  of  
the  PSIA  that  integrates  various  well-known  dynamic
loop  scheduling  (DLS)  techniques.  The  present  work:  (1)  Proposes  EPSIA,  
a  novel  flexible  version  of  PSIA;  (2)  Showcases
the   benefits   of   applying   DLS   techniques   for   optimizing   the
performance  of  the  PSIA;  (3)  Assesses  the  performance  of  the
proposed  EPSIA  by  conducting  several  scalability  experiments.
The  performance  results  are  promising  and  show  that  using
well-known   DLS   techniques,   the   performance   of   the   EPSIA
outperforms  the  performance  of  the  PSIA  by  a  factor  of  1.2
and 2 for homogeneous and heterogeneous computing resources, respectively.




*REF*
@INPROCEEDINGS{ParallelSpinimagesDynamicLScheduling, 
  author={A. Eleliemy and A. Mohammed and F. M. Ciorba}, 
  booktitle={2017 IEEE 19th International Conference on High Performance Computing and Communications Workshops (HPCCWS)}, 
  title={Efficient Generation of Parallel Spin-images Using Dynamic Loop Scheduling}, 
  year={2017}, 
  volume={}, 
  number={}, 
  pages={34-41}, 
  keywords={face recognition;object recognition;parallel processing;scheduling;stereo image processing;3D face recognition;3D object recognition;DLS techniques;EPSIA;PSIA;dynamic loop scheduling techniques;heterogeneous computing resources;high performance computing systems;homogeneous computing resources;modern HPC systems;novel flexible version;parallel spin-image algorithm;parallel spin-images;parallel version;Dynamic scheduling;Heterogeneous networks;Processor scheduling;Scalability;Three-dimensional displays;Dynamic-loop-scheduling;Efficient-performance;Factoring;Guided-self-scheduling;Self-scheduling;Spin-image-algorithm;Static-loop-scheduling}, 
  doi={10.1109/HPCCWS.2017.00012}, 
  ISSN={}, 
  month={Dec},
}

** 2018-07-06 Experimental Verification and Analysis ofDynamic Loop Scheduling in Scientific Applications 2018
In  thepresent work, a methodology is devised to answer this question.It  involves  the  experimental  verification  and  analysis  of  the
performance  of  DLS  in  scientific  applications.  The  proposedmethodology   is   employed   for   a   computer   vision   application
executing using four DLS techniques on two different HPC plat-forms, both via native and simulative experiments. The evaluation
and analysis of the native and simulative results indicate that theaccuracy of the simulative experiments is strongly influenced by
the  approach  used  to  extract  the  computational  effort  of  theapplication  (FLOP-  or  time-based),  the  choice  of  application
model representation into simulation (data or task parallel), and the available HPC subsystem models in the simulator (multi-core
CPUs, memory hierarchy, and network topology). The minimumand the maximum percent errors achieved between the native and
the  simulative  experiments  are 0.95% and 8.03%,  respectively.

Loop  scheduling.There  are  two  main  categories  of  loop scheduling techniques:  static and dynamic. The  essential 
difference between static and dynamic loop scheduling is the time when  the  scheduling  decisions  are  taken.  Static  scheduling
techniques, such as block, cyclic, and block-cyclic [10], divide and assign the loop iterations (or tasks) across the processing
elements  before  the  loop  executes.  The  tasks  division  andassignment do not change during execution. In this work, block
scheduling is considered and is denoted as STATIC. Dynamic  loop  scheduling  (DLS)  techniques  divide  and
self-schedule  the  loop  iterations  during  the  execution  of  theloop.  As  a  result,  DLS  techniques  balance  the  execution  of
the loop iterations at the cost of increased overhead comparedto the static techniques. 

*REF*
@article{mohammed2018experimental,
  title={Experimental Verification and Analysis of Dynamic Loop Scheduling in Scientific Applications},
  author={Mohammed, Ali and Eleliemy, Ahmed and Ciorba, Florina M and Kasielke, Franziska and Banicescu, Ioana},
  journal={arXiv preprint arXiv:1804.11115},
  year={2018}
}
* Weekly Reports
** 2018-05-17 - 2018-05-27
Studies about schedulers in general;
Reading of more papers, own research/papers from the Proposal;
Setting up my new environment, laptop etc;
Overview about benchmarks, nothing deeply studied yet;
Remembering my Master presentation.

--------
Review slides of performance analysis
Discuss...
** 2018-05-28 - 2018-06-04
Weekly report
This week I worked mainly on the CORAL 2 suit benchmark more specifically over qmcpack. Basically I decided to really start 
the task 1 now because before I was just looking around by the general literature.
** 2018-06-05 - 2018-06-07
The benchmarks suite as NAS and CORAL have a lot of applications.
What should I do, should I study each application? 
Like download it and go inside the code etc or just consider the papers about the applications.
** 2018-06-07 - 2018-06-11
Professor Florina, this week I decide to go back and study a bit more considering the references from the papers you sent me to read. 
I was not feeling well focusing on an aplicattion (CORAL -> QMCPACK, last week) without knowing exactly what  I was looking for.
I am heaving several doubts about the task 1 as a whole. Maybe we should talk about it.
Tomorrow I should send you a more detailed email about what are my questions and what is my proposal. I still thinking about it.


Think about the group "images".

Chapter 2 workload 
SPEC Benchmark
Consider Aplications with MPI and OPENMP
Overall Survey
LOOK WORKLOAD PPT

** 2018-06-11 - 2018-06-18
Professor Florina,
This week I worked on the table I send you with some features of each application from benchmark suites. It is not ready yet but it is going well, I am already selecting some benchmark applications 
for further and more detailed studies. As soon as I finish the table with the applications I intend to install them in our cluster just to be sure that they will be useful. 
Finally I want to deeply study them to get the information like scheduling type, "independent tasks and with data dependent tasks", "irregular task execution times" etc.


Next meetings - july 9 - 15h 

** 2018-06-18 - 2018-06-25 
I still working on the table but there are much more data to add. Last week I also spent some time translating the last documents that the university is requiring for my registration. 
I will give this to them tomorrow since they are already closed now and I also want to make an authenticated copy of one of the translations since it is a official one and costs a lot hehe.

PASC update: I have received my schedule for PASC and it is really good. I will be able to attend the poster session and I will be working on the room of 
"SPH-EXA: OPTIMIZING SMOOTH PARTICLE HYDRODYNAMICS FOR EXASCALE COMPUTING".

PASC schedule:
Monday, July 2, 2018 
07:45 - 09:00 Foyer 2 nd Floor Orientation session
12:40 – 15:00 Room Sydney > Help speakers

Tuesday, July 3, 2018
13:10 – 15:30 Room Nairobi > Help speakers 

Wednesday, July 4, 2018
11:00 – 13:15 Room Osaka > Help speakers
14:00 – 16:15 Room Sydney > Help speakers


For the next week I will keep working on the table.
2018-06-18 - 2018-07-04
So I still working on the table. For the next week I will have a better update. Probably I will start to study the applications in more detail. 
I think that I already have enough applications in the table and also these are the best documented ones.

* Visitors
** Prof. Allen Malony, University of Oregon 2018-06
TAU performance analisys.
Adaptive openmp loop scheduling
UPMLIB: A Runtime System for Tuning the Memory Performance of OpenMP Programs on Scalable Shared-Memory Multiprocessors 2000
* All BibTeX REFs

*REF*
@INPROCEEDINGS{ParallelSpinimagesDynamicLScheduling, 
  author={A. Eleliemy and A. Mohammed and F. M. Ciorba}, 
  booktitle={2017 IEEE 19th International Conference on High Performance Computing and Communications Workshops (HPCCWS)}, 
  title={Efficient Generation of Parallel Spin-images Using Dynamic Loop Scheduling}, 
  year={2017}, 
  volume={}, 
  number={}, 
  pages={34-41}, 
  keywords={face recognition;object recognition;parallel processing;scheduling;stereo image processing;3D face recognition;3D object recognition;DLS techniques;EPSIA;PSIA;dynamic loop scheduling techniques;heterogeneous computing resources;high performance computing systems;homogeneous computing resources;modern HPC systems;novel flexible version;parallel spin-image algorithm;parallel spin-images;parallel version;Dynamic scheduling;Heterogeneous networks;Processor scheduling;Scalability;Three-dimensional displays;Dynamic-loop-scheduling;Efficient-performance;Factoring;Guided-self-scheduling;Self-scheduling;Spin-image-algorithm;Static-loop-scheduling}, 
  doi={10.1109/HPCCWS.2017.00012}, 
  ISSN={}, 
  month={Dec},
}

*REF:*
@article{tran2018multi,
  title={Multi-stage resource-aware scheduling for data centers with heterogeneous servers},
  author={Tran, Tony T and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G and Beck, J Christopher},
  journal={Journal of Scheduling},
  volume={21},
  number={2},
  pages={251--267},
  year={2018},
  publisher={Springer}
}

*REF:*
@article{wang2014adaptive,
  title={An adaptive and hierarchical task scheduling scheme for multi-core clusters},
  author={Wang, Yizhuo and Zhang, Yang and Su, Yan and Wang, Xiaojun and Chen, Xu and Ji, Weixing and Shi, Feng},
  journal={Parallel Computing},
  volume={40},
  number={10},
  pages={611--627},
  year={2014},
  publisher={Elsevier}
}

*REF:*
@inproceedings{hansen2014heuristics,
  title={Heuristics for robust allocation of resources to parallel applications with uncertain execution times in heterogeneous systems with uncertain availability},
  author={Hansen, Timothy and Ciorba, Florina M and Maciejewski, Anthony A and Siegel, Howard Jay and Srivastava, Srishti and Banicescu, Ioana},
  booktitle={Proceedings of the World Congress on Engineering},
  volume={1},
  year={2014}
}

*REF:*
@inproceedings{ciorba2012combined,
  title={A combined dual-stage framework for robust scheduling of scientific applications in heterogeneous environments with uncertain availability},
  author={Ciorba, Florina M and Hansen, Timothy and Srivastava, Srishti and Banicescu, Ioana and Maciejewski, Anthony A and Siegel, Howard Jay},
  booktitle={Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW), 2012 IEEE 26th International},
  pages={193--207},
  year={2012},
  organization={IEEE}
}

*REF:*
@inproceedings{pilla2012hierarchical,
  title={A hierarchical approach for load balancing on parallel multi-core systems},
  author={Pilla, Laercio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V},
  booktitle={Parallel Processing (ICPP), 2012 41st International Conference on},
  pages={118--127},
  year={2012},
  organization={IEEE}
}

*REF:*
@article{pasquali2011multi,
  title={A multi-level scheduler for batch jobs on grids},
  author={Pasquali, Marco and Baraglia, Ranieri and Capannini, Gabriele and Ricci, Laura and Laforenza, Domenico},
  journal={The Journal of Supercomputing},
  volume={57},
  number={1},
  pages={81--98},
  year={2011},
  publisher={Springer}
}

*REF:*
@inproceedings{umale2010optimized,
  title={Optimized grid scheduling using two level decision algorithm (TLDA)},
  author={Umale, Jayant and Mahajan, Sunita},
  booktitle={Parallel Distributed and Grid Computing (PDGC), 2010 1st International Conference on},
  pages={78--82},
  year={2010},
  organization={IEEE}
}

*REF:*
@article{blelloch2007multiscale,
  title={Multiscale Scheduling: Integrating Competitive and Cooperative Scheduling in Theory and in Practice},
  author={Blelloch, Guy E and Blum, Lenore and Harchol-Balter, Mor and Harper, Robert},
  year={2007}
}

*REF:*
@article{kobayashi2001multiscale,
  title={Multiscale computing},
  author={Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
  journal={Proceedings of the National Academy of Sciences},
  volume={98},
  number={22},
  pages={12344--12345},
  year={2001},
  publisher={National Acad Sciences}
}

*REF:*
@article {Kobayashi12344,
	author = {Kobayashi, Mei and Irino, Toshio and Sweldens, Wim},
	title = {Multiscale computing},
	volume = {98},
	number = {22},
	pages = {12344--12345},
	year = {2001},
	doi = {10.1073/pnas.231384298},
	publisher = {National Academy of Sciences},
	abstract = {Multiscale computing (MSC) involves the computation, manipulation, and analysis of information at different resolution levels. Widespread use of MSC algorithms and the discovery of important relationships between different approaches to implementation were catalyzed, in part, by the recent interest in wavelets. We present two examples that demonstrate how MSC can help scientists understand complex data. The first is from acoustical signal processing and the second is from computer graphics. MSC,multiscale computing},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/98/22/12344},
	eprint = {http://www.pnas.org/content/98/22/12344.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

*REF:*
@article{huguet2000mixed,
  title={Mixed task scheduling and resource allocation problems},
  author={Huguet, Marie-Jos{\'e} and Lopez, Pierre},
  journal={Proceedings of CP-AI-OR’00, Paderborn, Germany},
  pages={71--79},
  year={2000}
}

*REF*
@INPROCEEDINGS{CombinedDualstageFrameworkScheduling, 
author={F. M. Ciorba and T. Hansen and S. Srivastava and I. Banicescu and A. A. Maciejewski and H. J. Siegel}, 
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum}, 
title={A Combined Dual-stage Framework for Robust Scheduling of Scientific Applications in Heterogeneous Environments with Uncertain Availability}, 
year={2012}, 
volume={}, 
number={}, 
pages={193-207}, 
keywords={natural sciences computing;parallel processing;probability;resource allocation;scheduling;combined dual-stage framework;dynamic load balancing;heterogeneous computing environments;parallel application scheduling;probability maximization;robust dynamic loop scheduling techniques;robust resource allocation heuristics;scientific applications;system make span minimization;uncertain availability;Availability;Dynamic scheduling;Program processors;Resource management;Robustness;Runtime;Uncertainty;dynamic loop scheduling;heterogeneous systems;high performance;non-dedicated systems;resource allocation;robustness;uncertainties}, 
doi={10.1109/IPDPSW.2012.5}, 
ISSN={}, 
month={May},}

*REF*
@INPROCEEDINGS{TwolevelSchedulingAhmed, 
author={A. Eleliemy and A. Mohammed and F. M. Ciorba}, 
booktitle={2017 16th International Symposium on Parallel and Distributed Computing (ISPDC)}, 
title={Exploring the Relation between Two Levels of Scheduling Using a Novel Simulation Approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={26-33}, 
keywords={parallel processing;scheduling;application scheduling algorithms;batch scheduling;modern high performance computing systems;novel simulation approach;respective level;scheduling levels;two-level simulator;Computational modeling;Hardware;Scheduling;Scheduling algorithms;Alea;Application level scheduling;Batch level scheduling;GridSim;High performance computing;OTF2;SimDag;SimGrid;Two-level scheduling;Vampir.}, 
doi={10.1109/ISPDC.2017.23}, 
ISSN={}, 
month={July},}

* CANCELLED Tools list MLS                                       :deprecated:
- State "CANCELLED"  from              [2018-06-21 Do 16:28]
** SMB-1
Spark Multi-User Benchmark, SMB, v. 1, 2016
hub.jazz.net/project/pc4spark/SparkMulti-UserBenchmark-1
Measures resource manager performance for Spark (data analysis) workloads in a multi-user scenario
Simulates multiple users submitting short-duration jobs concurrently to systems managed by a resource manager (such as Apache YARN, Apache Mesos, or IBM Platform Conductor for Spark)
** The Hardware Accelerated Cosmology Code (HACC)
The code is hybrid MPI-OpenMP and depends on external FFT library

** Check!
chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/http://www-mount.ece.umn.edu/~jjyi/MoBS/2009/program/02E-Bienia.pdf
